# -*- coding: utf-8 -*-
"""BOW_with_basic_features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WlSgG7xB7npmGWpOJ5YLRlamfHqks8py
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('train.csv')
df.shape

"""# INITIAL ANALYSIS"""

new_df = df.sample(30000,random_state=2)

new_df.isnull().sum()

new_df.duplicated().sum()

print(new_df["is_duplicate"].value_counts())
print((new_df["is_duplicate"].value_counts()/new_df["is_duplicate"].count())*100)
new_df['is_duplicate'].value_counts().plot(kind='bar')

"""Getting no of unique and repeated questions"""

qid = pd.Series(new_df["qid1"].tolist() + new_df['qid2'].tolist())
print("Number of unique questions",np.unique(qid).shape[0])
x = qid.value_counts()>1
print('Number of questions getting repeated',x[x].shape[0])

plt.hist(qid.value_counts().values,bins = 160)
plt.yscale('log')
plt.show()

"""# **ADDING FEATURES**


1.   q1_len
2.   q2_len
3. q1_words
4. q2_words
5. words common
6. words total
7. word share




"""

# feature engineering
new_df['q1_len'] = new_df['question1'].str.len()
new_df['q2_len'] = new_df['question2'].str.len()

new_df.head()

new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(" ")))
new_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(" ")))
new_df.head()

def common_words(row):
  w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(" ")))
  w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(" ")))
  return len(w1 & w2)

new_df['word_common'] = new_df.apply(common_words,axis=1)
new_df.head()

def total_words(row):
  w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(" ")))
  w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(" ")))
  return (len(w1) + len(w2))

new_df['word_total'] = new_df.apply(total_words,axis=1)
new_df.head()

new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)
new_df.head()

"""# **EDA ANALYSIS OF FEATURES**"""

# Analysis of features
sns.displot(new_df['q1_len'])
print('minimum characters',new_df['q1_len'].min())
print('maximum characters',new_df['q1_len'].max())
print('average num of characters',int(new_df['q1_len'].mean()))

sns.displot(new_df['q2_len'])
print('minimum characters',new_df['q2_len'].min())
print('maximum characters',new_df['q2_len'].max())
print('average num of characters',int(new_df['q2_len'].mean()))

sns.displot(new_df['q1_num_words'])
print('minimum characters',new_df['q1_num_words'].min())
print('maximum characters',new_df['q1_num_words'].max())
print('average num of characters',int(new_df['q1_num_words'].mean()))

sns.displot(new_df['q2_num_words'])
print('minimum characters',new_df['q2_num_words'].min())
print('maximum characters',new_df['q2_num_words'].max())
print('average num of characters',int(new_df['q2_num_words'].mean()))

# common words
sns.distplot(new_df[new_df['is_duplicate'] == 0]['word_common'],label = 'non_duplicate')
sns.distplot(new_df[new_df['is_duplicate'] == 1]['word_common'],label = 'duplicate')
plt.legend()
plt.show()

# total words
sns.distplot(new_df[new_df['is_duplicate'] == 0]['word_total'],label = 'non_duplicate')
sns.distplot(new_df[new_df['is_duplicate'] == 1]['word_total'],label = 'duplicate')
plt.legend()
plt.show()

# word share
sns.distplot(new_df[new_df['is_duplicate'] == 0]['word_share'],label = 'non_duplicate')
sns.distplot(new_df[new_df['is_duplicate'] == 1]['word_share'],label = 'duplicate')
plt.legend()
plt.show()

ques_df = new_df[['question1','question2']]
ques_df.head()

final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])
print(final_df.shape)
final_df.head()

from sklearn.feature_extraction.text import CountVectorizer
# merge texts
questions = list(ques_df["question1"]) + list(ques_df['question2'])

cv = CountVectorizer(max_features=3000)
q1_arr , q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)

temp_df1 = pd.DataFrame(q1_arr,index= ques_df.index)
temp_df2 = pd.DataFrame(q2_arr,index= ques_df.index)
temp_df = pd.concat([temp_df1,temp_df2],axis=1)
temp_df.shape

final_df = pd.concat([final_df, temp_df],axis=1)
print(final_df.shape)
final_df.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size = 0.2,random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
rf = RandomForestClassifier()
rf.fit(X_train,y_train)
y_pred = rf.predict(X_test)
accuracy_score(y_test,y_pred)

from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb.fit(X_train,y_train)
y_pred = xgb.predict(X_test)
accuracy_score(y_test,y_pred)

